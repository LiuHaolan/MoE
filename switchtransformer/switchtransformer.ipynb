{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from typing import Tuple\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import dataset\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Module):\n",
    "    \"\"\"\n",
    "    ## FFN module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 activation=nn.ReLU(),\n",
    "                 is_gated: bool = False,\n",
    "                 bias1: bool = True,\n",
    "                 bias2: bool = True,\n",
    "                 bias_gate: bool = True):\n",
    "        \"\"\"\n",
    "        * `d_model` is the number of features in a token embedding\n",
    "        * `d_ff` is the number of features in the hidden layer of the FFN\n",
    "        * `dropout` is dropout probability for the hidden layer\n",
    "        * `is_gated` specifies whether the hidden layer is gated\n",
    "        * `bias1` specified whether the first fully connected layer should have a learnable bias\n",
    "        * `bias2` specified whether the second fully connected layer should have a learnable bias\n",
    "        * `bias_gate` specified whether the fully connected layer for the gate should have a learnable bias\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Layer one parameterized by weight $W_1$ and bias $b_1$\n",
    "        self.layer1 = nn.Linear(d_model, d_ff, bias=bias1)\n",
    "        # Layer one parameterized by weight $W_1$ and bias $b_1$\n",
    "        self.layer2 = nn.Linear(d_ff, d_model, bias=bias2)\n",
    "        # Hidden layer dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Activation function $f$\n",
    "        self.activation = activation\n",
    "        # Whether there is a gate\n",
    "        self.is_gated = is_gated\n",
    "        if is_gated:\n",
    "            # If there is a gate the linear layer to transform inputs to\n",
    "            # be multiplied by the gate, parameterized by weight $V$ and bias $c$\n",
    "            self.linear_v = nn.Linear(d_model, d_ff, bias=bias_gate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # $f(x W_1 + b_1)$\n",
    "        g = self.activation(self.layer1(x))\n",
    "        # If gated, $f(x W_1 + b_1) \\otimes (x V + b) $\n",
    "        if self.is_gated:\n",
    "            x = g * self.linear_v(x)\n",
    "        # Otherwise\n",
    "        else:\n",
    "            x = g\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # $(f(x W_1 + b_1) \\otimes (x V + b)) W_2 + b_2$ or $f(x W_1 + b_1) W_2 + b_2$\n",
    "        # depending on whether it is gated\n",
    "        return self.layer2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SwitchFeedForward(Module):\n",
    "    \"\"\"\n",
    "    ## Routing among multiple FFNs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *,\n",
    "                 capacity_factor: float,\n",
    "                 drop_tokens: bool,\n",
    "                 is_scale_prob: bool,\n",
    "                 n_experts: int,\n",
    "                 expert: FeedForward,\n",
    "                 d_model: int):\n",
    "        \"\"\"\n",
    "        * `capacity_factor` is the capacity of each expert as a factor relative to ideally balanced load\n",
    "        * `drop_tokens` specifies whether to drop tokens if more tokens are routed to an expert than the capacity\n",
    "        * `is_scale_prob` specifies whether to multiply the input to the FFN by the routing probability\n",
    "        * `n_experts` is the number of experts\n",
    "        * `expert` is the expert layer, a [FFN module](../feed_forward.html)\n",
    "        * `d_model` is the number of features in a token embedding\n",
    "        * `d_ff` is the number of features in the hidden layer of the FFN\n",
    "        * `dropout` is dropout probability in the FFN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.is_scale_prob = is_scale_prob\n",
    "        self.n_experts = n_experts\n",
    "        self.drop_tokens = drop_tokens\n",
    "\n",
    "        # make copies of the FFNs\n",
    "        self.experts = [expert for i in range(n_experts)]\n",
    "        # Routing layer and softmax\n",
    "        self.switch = nn.Linear(d_model, n_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the input to the switching module with shape `[seq_len, batch_size, d_model]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Capture the shape to change shapes later\n",
    "        seq_len, batch_size, d_model = x.shape\n",
    "        # Flatten the sequence and batch dimensions\n",
    "        x = x.view(-1, d_model)\n",
    "\n",
    "        # Get routing probabilities for each of the tokens.\n",
    "        # $$p_i(x) = \\frac{e^{h(x)_i}}{\\sum^N_j e^{h(x)_j}}$$\n",
    "        # where $N$ is the number of experts `n_experts` and\n",
    "        # $h(\\cdot)$ is the linear transformation of token embeddings.\n",
    "        route_prob = self.softmax(self.switch(x))\n",
    "\n",
    "        # Get the maximum routing probabilities and the routes.\n",
    "        # We route to the expert with highest probability\n",
    "        route_prob_max, routes = torch.max(route_prob, dim=-1)\n",
    "\n",
    "        # Get indexes of tokens going to each expert\n",
    "        indexes_list = [torch.eq(routes, i).nonzero(as_tuple=True)[0] for i in range(self.n_experts)]\n",
    "\n",
    "        # Initialize an empty tensor to store outputs\n",
    "        final_output = x.new_zeros(x.shape)\n",
    "\n",
    "        # Capacity of each expert.\n",
    "        # $$\\mathrm{expert\\;capacity} =\n",
    "        # \\frac{\\mathrm{tokens\\;per\\;batch}}{\\mathrm{number\\;of\\;experts}}\n",
    "        # \\times \\mathrm{capacity\\;factor}$$\n",
    "        capacity = int(self.capacity_factor * len(x) / self.n_experts)\n",
    "        # Number of tokens routed to each expert.\n",
    "        counts = x.new_tensor([len(indexes_list[i]) for i in range(self.n_experts)])\n",
    "\n",
    "        # Initialize an empty list of dropped tokens\n",
    "        dropped = []\n",
    "        # Only drop tokens if `drop_tokens` is `True`.\n",
    "        if self.drop_tokens:\n",
    "            # Drop tokens in each of the experts\n",
    "            for i in range(self.n_experts):\n",
    "                # Ignore if the expert is not over capacity\n",
    "                if len(indexes_list[i]) <= capacity:\n",
    "                    continue\n",
    "                # Shuffle indexes before dropping\n",
    "                indexes_list[i] = indexes_list[i][torch.randperm(len(indexes_list[i]))]\n",
    "                # Collect the tokens over capacity as dropped tokens\n",
    "                dropped.append(indexes_list[i][capacity:])\n",
    "                # Keep only the tokens upto the capacity of the expert\n",
    "                indexes_list[i] = indexes_list[i][:capacity]\n",
    "\n",
    "        # Get outputs of the expert FFNs\n",
    "        expert_output = [self.experts[i](x[indexes_list[i], :]) for i in range(self.n_experts)]\n",
    "\n",
    "        # Assign to final output\n",
    "        for i in range(self.n_experts):\n",
    "            final_output[indexes_list[i], :] = expert_output[i]\n",
    "\n",
    "        # Pass through the dropped tokens\n",
    "        if dropped:\n",
    "            dropped = torch.cat(dropped)\n",
    "            final_output[dropped, :] = x[dropped, :]\n",
    "\n",
    "        if self.is_scale_prob:\n",
    "            # Multiply by the expert outputs by the probabilities $y = p_i(x) E_i(x)$\n",
    "            final_output = final_output * route_prob_max.view(-1, 1)\n",
    "        else:\n",
    "            # Don't scale the values but multiply by $\\frac{p}{\\hat{p}} = 1$ so that the gradients flow\n",
    "            # (this is something we experimented with).\n",
    "            final_output = final_output * (route_prob_max / route_prob_max.detach()).view(-1, 1)\n",
    "\n",
    "        # Change the shape of the final output back to `[seq_len, batch_size, d_model]`\n",
    "        final_output = final_output.view(seq_len, batch_size, d_model)\n",
    "\n",
    "        # Return\n",
    "        #\n",
    "        # * the final output\n",
    "        # * number of tokens routed to each expert\n",
    "        # * sum of probabilities for each expert\n",
    "        # * number of tokens dropped.\n",
    "        # * routing probabilities of the selected experts\n",
    "        #\n",
    "        # These are used for the load balancing loss and logging\n",
    "        return final_output, counts, route_prob.sum(0), len(dropped), route_prob_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchTransformerLayer(Module):\n",
    "    \"\"\"\n",
    "    # Switch Transformer Block\n",
    "    This is the same as [normal transformer block](../models.html#TransformerLayer)\n",
    "    with handling extra outputs of switch feedforward module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *,\n",
    "                 d_model: int,\n",
    "                 attn: MultiheadAttention,\n",
    "                 feed_forward: SwitchFeedForward,\n",
    "                 dropout_prob: float):\n",
    "        \"\"\"\n",
    "        * `d_model` is the token embedding size\n",
    "        * `attn` is the attention module\n",
    "        * `feed_forward` is the feed forward module (which is the switching module in this case)\n",
    "        * `dropout_prob` is the probability of dropping out after self attention and FFN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.attn = attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.norm_self_attn = nn.LayerNorm([d_model])\n",
    "        self.norm_ff = nn.LayerNorm([d_model])\n",
    "\n",
    "    def forward(self, *,\n",
    "                x: torch.Tensor,\n",
    "                mask: torch.Tensor):\n",
    "        # Normalize the vectors before doing self attention\n",
    "        z = self.norm_self_attn(x)\n",
    "        # Run through self attention, i.e. keys and values are from self\n",
    "        self_attn, _ = self.attn(query=z, key=z, value=z, attn_mask=mask)\n",
    "        # Add the self attention results\n",
    "\n",
    "        x = x + self.dropout(self_attn)\n",
    "\n",
    "        # Normalize for feed-forward\n",
    "        z = self.norm_ff(x)\n",
    "        # Pass through the switching feed-forward network\n",
    "        ff, counts, route_prob, n_dropped, route_prob_max = self.feed_forward(z)\n",
    "        # Add the feed-forward results back\n",
    "        x = x + self.dropout(ff)\n",
    "\n",
    "        return x, counts, route_prob, n_dropped, route_prob_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class SwitchTransformer(Module):\n",
    "    \"\"\"\n",
    "    ## Switch Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer: SwitchTransformerLayer, n_layers: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # Make copies of the transformer layer\n",
    "        self.layers = [layer for _ in range(n_layers)]\n",
    "        # Final normalization layer\n",
    "        self.norm = nn.LayerNorm([layer.size])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        # Run through each transformer layer\n",
    "        counts, route_prob, n_dropped, route_prob_max = [], [], [], []       \n",
    "        for layer in self.layers:\n",
    "            x, f, p, n_d, p_max = layer(x=x, mask=mask)\n",
    "            counts.append(f)\n",
    "            route_prob.append(p)\n",
    "            n_dropped.append(n_d)\n",
    "            route_prob_max.append(p_max)\n",
    "        # Finally, normalize the vectors\n",
    "        x = self.norm(x)\n",
    "        #\n",
    "        return x, torch.stack(counts), torch.stack(route_prob), n_dropped, torch.stack(route_prob_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchTransformerModel(Module):\n",
    "    \"\"\"\n",
    "    ## Auto regressive model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_vocab: int, d_model: int, transformer: Module):\n",
    "        super().__init__()\n",
    "        # Token embedding module\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.src_embed = nn.Embedding(n_vocab, d_model)\n",
    "        # Transformer\n",
    "        self.transformer = transformer\n",
    "        # Final layer\n",
    "        self.generator = nn.Linear(d_model, n_vocab)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        # Initialize the subsequent mask\n",
    "        # Token embeddings\n",
    "        x = self.src_embed(x)\n",
    "        # Run it through the transformer\n",
    "        res, counts, route_prob, n_dropped, route_prob_max = self.transformer(x, mask)\n",
    "        # Generate logits of the next token\n",
    "        res = self.generator(res)\n",
    "        #\n",
    "        return res\n",
    "        # return res, counts, route_prob, n_dropped, route_prob_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token embedding size\n",
    "n_tokens: int = len(vocab)\n",
    "# Number of attention heads\n",
    "d_model: int = 200\n",
    "heads: int = 2\n",
    "# Dropout probability\n",
    "dropout: float = 0.0\n",
    "# Number of features in FFN hidden layer\n",
    "d_ff: int = 200\n",
    "# Number of transformer layers\n",
    "n_layers: int = 6\n",
    "# Number of experts\n",
    "n_experts: int = 4\n",
    "# Load balancing coefficient\n",
    "load_balancing_loss_ceof = 0.01\n",
    "# Whether to scale the chosen expert outputs by the routing probability\n",
    "is_scale_prob: bool = True\n",
    "# Whether to drop tokens\n",
    "drop_tokens: bool = False\n",
    "# Capacity factor to determine capacity of each model\n",
    "capacity_factor: float = 1.0\n",
    "    \n",
    "    \n",
    "# why d_model divide heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwitchTransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (src_embed): Embedding(28782, 200)\n",
       "  (transformer): SwitchTransformer(\n",
       "    (norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (generator): Linear(in_features=200, out_features=28782, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model1 = SwitchTransformer(\n",
    "        SwitchTransformerLayer(d_model=d_model,\n",
    "                               attn=MultiheadAttention( d_model, heads, dropout),\n",
    "                               feed_forward=SwitchFeedForward(capacity_factor=capacity_factor,\n",
    "                                                              drop_tokens=drop_tokens,\n",
    "                                                              is_scale_prob=is_scale_prob,\n",
    "                                                              n_experts=n_experts,\n",
    "                                                              expert=FeedForward(d_model, d_ff, dropout),\n",
    "                                                              d_model=d_model),\n",
    "                               dropout_prob=dropout),\n",
    "        n_layers)\n",
    "\n",
    "\n",
    "model = SwitchTransformerModel(n_tokens, d_model, model1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 150.90 | loss  7.50 | ppl  1806.44\n"
     ]
    }
   ],
   "source": [
    "# training code\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "            \n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
